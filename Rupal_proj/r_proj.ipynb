{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Internship Interview Project\n",
    "\n",
    "Topic : Building a  ML model to analyse the customer sentiments from kindle book reviews.\n",
    "\n",
    "For this task I have used tensorflow as my primary model building and testing module."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset I have used here is the openly available data of amazon kindle book reviews. This dataset consists of the identifier of the book, the book review, the reviewer name, rating, review time and other information. Out of these the ones I will be focusing on is the review and the rating. The goal is to classify the customer sentiment into positive and negative based on the rating and then build and train a model to learn the customer sentiment from the review text itself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing necessary libraries\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Embedding, LSTM, SimpleRNN, GRU\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0.1</th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>asin</th>\n",
       "      <th>helpful</th>\n",
       "      <th>rating</th>\n",
       "      <th>reviewText</th>\n",
       "      <th>reviewTime</th>\n",
       "      <th>reviewerID</th>\n",
       "      <th>reviewerName</th>\n",
       "      <th>summary</th>\n",
       "      <th>unixReviewTime</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>11539</td>\n",
       "      <td>B0033UV8HI</td>\n",
       "      <td>[8, 10]</td>\n",
       "      <td>3</td>\n",
       "      <td>Jace Rankin may be short, but he's nothing to ...</td>\n",
       "      <td>09 2, 2010</td>\n",
       "      <td>A3HHXRELK8BHQG</td>\n",
       "      <td>Ridley</td>\n",
       "      <td>Entertaining But Average</td>\n",
       "      <td>1283385600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>5957</td>\n",
       "      <td>B002HJV4DE</td>\n",
       "      <td>[1, 1]</td>\n",
       "      <td>5</td>\n",
       "      <td>Great short read.  I didn't want to put it dow...</td>\n",
       "      <td>10 8, 2013</td>\n",
       "      <td>A2RGNZ0TRF578I</td>\n",
       "      <td>Holly Butler</td>\n",
       "      <td>Terrific menage scenes!</td>\n",
       "      <td>1381190400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>9146</td>\n",
       "      <td>B002ZG96I4</td>\n",
       "      <td>[0, 0]</td>\n",
       "      <td>3</td>\n",
       "      <td>I'll start by saying this is the first of four...</td>\n",
       "      <td>04 11, 2014</td>\n",
       "      <td>A3S0H2HV6U1I7F</td>\n",
       "      <td>Merissa</td>\n",
       "      <td>Snapdragon Alley</td>\n",
       "      <td>1397174400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>7038</td>\n",
       "      <td>B002QHWOEU</td>\n",
       "      <td>[1, 3]</td>\n",
       "      <td>3</td>\n",
       "      <td>Aggie is Angela Lansbury who carries pocketboo...</td>\n",
       "      <td>07 5, 2014</td>\n",
       "      <td>AC4OQW3GZ919J</td>\n",
       "      <td>Cleargrace</td>\n",
       "      <td>very light murder cozy</td>\n",
       "      <td>1404518400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>1776</td>\n",
       "      <td>B001A06VJ8</td>\n",
       "      <td>[0, 1]</td>\n",
       "      <td>4</td>\n",
       "      <td>I did not expect this type of book to be in li...</td>\n",
       "      <td>12 31, 2012</td>\n",
       "      <td>A3C9V987IQHOQD</td>\n",
       "      <td>Rjostler</td>\n",
       "      <td>Book</td>\n",
       "      <td>1356912000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0.1  Unnamed: 0        asin  helpful  rating  \\\n",
       "0             0       11539  B0033UV8HI  [8, 10]       3   \n",
       "1             1        5957  B002HJV4DE   [1, 1]       5   \n",
       "2             2        9146  B002ZG96I4   [0, 0]       3   \n",
       "3             3        7038  B002QHWOEU   [1, 3]       3   \n",
       "4             4        1776  B001A06VJ8   [0, 1]       4   \n",
       "\n",
       "                                          reviewText   reviewTime  \\\n",
       "0  Jace Rankin may be short, but he's nothing to ...   09 2, 2010   \n",
       "1  Great short read.  I didn't want to put it dow...   10 8, 2013   \n",
       "2  I'll start by saying this is the first of four...  04 11, 2014   \n",
       "3  Aggie is Angela Lansbury who carries pocketboo...   07 5, 2014   \n",
       "4  I did not expect this type of book to be in li...  12 31, 2012   \n",
       "\n",
       "       reviewerID  reviewerName                   summary  unixReviewTime  \n",
       "0  A3HHXRELK8BHQG        Ridley  Entertaining But Average      1283385600  \n",
       "1  A2RGNZ0TRF578I  Holly Butler   Terrific menage scenes!      1381190400  \n",
       "2  A3S0H2HV6U1I7F       Merissa          Snapdragon Alley      1397174400  \n",
       "3   AC4OQW3GZ919J    Cleargrace    very light murder cozy      1404518400  \n",
       "4  A3C9V987IQHOQD      Rjostler                      Book      1356912000  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "#load the data \n",
    "data = pd.read_csv(r\"C:\\Users\\gervi\\Desktop\\Rupal_proj\\archive\\all_kindle_review .csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To classify the customer sentiment into positive and negative, I have use the book rating as a proxy. Here I have defined the sentiment as follows: a rating of 3 or above is labelled positive (1) and a lower rating yields a negative sentiment (0).\n",
    "\n",
    "\n",
    "Later the dataset is split into training and test set where test set constitutes 20% of the full dataset. Training model needs to be large to ensure good optimization of the model parameters.\n",
    "\n",
    "To build the model, a relationship between the review text and the customer sentiment needs to be established. For this I first assign indices or tokens to the most frequently used words in the review text of the training set. Here a limit of 10000 most frequent words was chosen by me. This means that the most freqeunt word will have the token '1', the next one '2' and so on. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#defining the sentiment, `1` for positive and `0` for negative based on the customer rating (here 3 is chosen as a threshold)\n",
    "data['sentiment'] = data['rating'].apply(lambda rating : +1 if rating >= 3 else 0)\n",
    "\n",
    "\n",
    "#splitting the data into training and testing sets, here reviewText is the feature and sentiment is the target\n",
    "X_train, X_test, y_train, y_test = train_test_split(data['reviewText'], data['sentiment'], test_size=0.2, random_state=2)\n",
    "\n",
    "\n",
    "#defining the maximum number of words to be used in the model and the maximum length of the review to be considered\n",
    "nwords_max = 10000\n",
    "len_max = 500\n",
    "\n",
    "#This is to assing index or tokens to each word in the review based on its frequency, here the 10000 most frequent words are collected\n",
    "tokenizer = tf.keras.preprocessing.text.Tokenizer(num_words=nwords_max)\n",
    "tokenizer.fit_on_texts(X_train)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next part I convert the review text into a sequence of tokens that were assigned in the previous block of code and the length of all reviews is truncated or padded to 500 units. This step converts our review text into sequences of length 500, which is later used to establish a relation with the sentiment (0 or 1)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now the words in the reviews are converted to sequence of indices or tokens based on the previous tokenization step for the training and test samples\n",
    "X_train_seq = tokenizer.texts_to_sequences(X_train)\n",
    "X_test_seq = tokenizer.texts_to_sequences(X_test)\n",
    "\n",
    "\n",
    "#to ensure that each review is of the same length, the reviews are padded with zeros to make them of the same length i.e in this case\n",
    "X_train_seq_trunc = tf.keras.preprocessing.sequence.pad_sequences(X_train_seq, maxlen=len_max)\n",
    "X_test_seq_trunc = tf.keras.preprocessing.sequence.pad_sequences(X_test_seq, maxlen=len_max)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The most important step is now to build a model based on the nature of relationship between our feature (review text) and target (sentiment). For this purpose I decide to use a simple sequential model with a few add-on layers. The activation funtion is chosen as sigmoid for the obvious binary nature of the target."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\gervi\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\layers\\core\\embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "#Building the model\n",
    "\n",
    "#Here we are using a simple sequential model upon which an embedding layer, LSTM layer(for sentences) and a dense layer is added\n",
    "kindle_model = Sequential()\n",
    "kindle_model.add(Embedding(nwords_max,128,input_length=len_max))\n",
    "kindle_model.add(LSTM(64,dropout=0.2, recurrent_dropout=0.2))\n",
    "kindle_model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "kindle_model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this crucial step I fit the model with my training sample set in batches of size 64 over 5 epochs and later use the test set to validate the model. The accuracy of the model is also calculated at each epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m150/150\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m49s\u001b[0m 324ms/step - accuracy: 0.7438 - loss: 0.5056 - val_accuracy: 0.8200 - val_loss: 0.3990\n",
      "Epoch 2/5\n",
      "\u001b[1m150/150\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m50s\u001b[0m 330ms/step - accuracy: 0.8645 - loss: 0.3250 - val_accuracy: 0.8462 - val_loss: 0.3691\n",
      "Epoch 3/5\n",
      "\u001b[1m150/150\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m51s\u001b[0m 338ms/step - accuracy: 0.9080 - loss: 0.2384 - val_accuracy: 0.8383 - val_loss: 0.3846\n",
      "Epoch 4/5\n",
      "\u001b[1m150/150\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m51s\u001b[0m 339ms/step - accuracy: 0.9292 - loss: 0.1933 - val_accuracy: 0.8308 - val_loss: 0.4250\n",
      "Epoch 5/5\n",
      "\u001b[1m150/150\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m50s\u001b[0m 335ms/step - accuracy: 0.9353 - loss: 0.1798 - val_accuracy: 0.8350 - val_loss: 0.4901\n",
      "Epoch 1/5\n",
      "\u001b[1m150/150\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m50s\u001b[0m 333ms/step - accuracy: 0.9512 - loss: 0.1325 - val_accuracy: 0.8221 - val_loss: 0.4737\n",
      "Epoch 2/5\n",
      "\u001b[1m150/150\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m50s\u001b[0m 333ms/step - accuracy: 0.9577 - loss: 0.1200 - val_accuracy: 0.8196 - val_loss: 0.5500\n",
      "Epoch 3/5\n",
      "\u001b[1m150/150\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m49s\u001b[0m 330ms/step - accuracy: 0.9322 - loss: 0.1861 - val_accuracy: 0.8196 - val_loss: 0.5119\n",
      "Epoch 4/5\n",
      "\u001b[1m150/150\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m51s\u001b[0m 339ms/step - accuracy: 0.9557 - loss: 0.1279 - val_accuracy: 0.8150 - val_loss: 0.5723\n",
      "Epoch 5/5\n",
      "\u001b[1m150/150\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m61s\u001b[0m 405ms/step - accuracy: 0.9670 - loss: 0.0949 - val_accuracy: 0.8104 - val_loss: 0.6172\n"
     ]
    }
   ],
   "source": [
    "#fitting the model\n",
    "\n",
    "kindle_model.fit(X_train_seq_trunc, y_train, batch_size=64, epochs=5, validation_data=(X_test_seq_trunc1, y_test1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 60ms/step - accuracy: 0.8291 - loss: 0.4630\n",
      "test accuracy:  0.8374999761581421\n"
     ]
    }
   ],
   "source": [
    "#to evaluate the performance of the model on the test data set the loss and accuracy are calculated\n",
    "score, accu = kindle_model.evaluate(X_test_seq_trunc, y_test, batch_size=64)\n",
    "print('test accuracy: ', accu)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The accuracy of this model is now obtained which is around 0.84. This indicated that our simple model is fairly good at gauging the customer sentiment just from the text written in the review. Such a model would be very useful to make useful and practical product decisions for the business to make sure they are adhering to the feedback received from the customers in general. Specifically in this case of Amazon Kindle, such a ML model would help simply and understand customer sentiment for different genres of book and along with the demographic information of the customer can aid make informed business decisions. \n",
    "\n",
    "As for the model and its evaluation, a lot more can be build up on this project like using different models and playing around with the hyperparameters like epochs, maximum number of word etc. Also for the evaluation of the model a confusion matrix or a loss function curve can be computed as well to help visualize the fitting better. However for the sake of this project I have decided to keep it simple and concise. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
