{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Internship Interview Project\n",
    "\n",
    "Topic : Building a  ML model to analyse the customer sentiments from kindle book reviews.\n",
    "\n",
    "For this task I have used tensorflow as my primary model building and testing module."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset I have used here is the openly available data of amazon kindle book reviews. This dataset consists of the identifier of the book, the book review, the reviewer name, rating, review time and other information. Out of these the ones I will be focusing on is the review and the rating. The goal is to classify the customer sentiment into positive and negative based on the rating and then build and train a model to learn the customer sentiment from the review text itself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing necessary libraries\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Embedding, LSTM, SimpleRNN, GRU\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#load the data \n",
    "data = pd.read_csv(r\"archive\\all_kindle_review .csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To classify the customer sentiment into positive and negative, I have use the book rating as a proxy. Here I have defined the sentiment as follows: a rating of 3 or above is labelled positive (1) and a lower rating yields a negative sentiment (0).\n",
    "\n",
    "\n",
    "Later the dataset is split into training and test set where test set constitutes 20% of the full dataset. Training model needs to be large to ensure good optimization of the model parameters.\n",
    "\n",
    "To build the model, a relationship between the review text and the customer sentiment needs to be established. For this I first assign indices or tokens to the most frequently used words in the review text of the training set. Here a limit of 10000 most frequent words was chosen by me. This means that the most freqeunt word will have the token '1', the next one '2' and so on. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#defining the sentiment, `1` for positive and `0` for negative based on the customer rating (here 3 is chosen as a threshold)\n",
    "data['sentiment'] = data['rating'].apply(lambda rating : +1 if rating >= 3 else 0)\n",
    "\n",
    "\n",
    "#splitting the data into training and testing sets, here reviewText is the feature and sentiment is the target\n",
    "X_train, X_test, y_train, y_test = train_test_split(data['reviewText'], data['sentiment'], test_size=0.2, random_state=2)\n",
    "\n",
    "\n",
    "#defining the maximum number of words to be used in the model and the maximum length of the review to be considered\n",
    "nwords_max = 10000\n",
    "len_max = 500\n",
    "\n",
    "#This is to assing index or tokens to each word in the review based on its frequency, here the 10000 most frequent words are collected\n",
    "tokenizer = tf.keras.preprocessing.text.Tokenizer(num_words=nwords_max)\n",
    "tokenizer.fit_on_texts(X_train)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next part I convert the review text into a sequence of tokens that were assigned in the previous block of code and the length of all reviews is truncated or padded to 500 units. This step converts our review text into sequences of length 500, which is later used to establish a relation with the sentiment (0 or 1)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now the words in the reviews are converted to sequence of indices or tokens based on the previous tokenization step for the training and test samples\n",
    "X_train_seq = tokenizer.texts_to_sequences(X_train)\n",
    "X_test_seq = tokenizer.texts_to_sequences(X_test)\n",
    "\n",
    "\n",
    "#to ensure that each review is of the same length, the reviews are padded with zeros to make them of the same length i.e in this case\n",
    "X_train_seq_trunc = tf.keras.preprocessing.sequence.pad_sequences(X_train_seq, maxlen=len_max)\n",
    "X_test_seq_trunc = tf.keras.preprocessing.sequence.pad_sequences(X_test_seq, maxlen=len_max)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The most important step is now to build a model based on the nature of relationship between our feature (review text) and target (sentiment). For this purpose I decide to use a simple sequential model with a few add-on layers. The activation funtion is chosen as sigmoid for the obvious binary nature of the target."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\gervi\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\layers\\core\\embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "#Building the model\n",
    "\n",
    "#Here we are using a simple sequential model upon which an embedding layer, LSTM layer(for sentences) and a dense layer is added\n",
    "kindle_model = Sequential()\n",
    "kindle_model.add(Embedding(nwords_max,128,input_length=len_max))\n",
    "kindle_model.add(LSTM(64,dropout=0.2, recurrent_dropout=0.2))\n",
    "kindle_model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "kindle_model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this crucial step I fit the model with my training sample set in batches of size 64 over 5 epochs and later use the test set to validate the model. The accuracy of the model is also calculated at each epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m150/150\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m49s\u001b[0m 324ms/step - accuracy: 0.7438 - loss: 0.5056 - val_accuracy: 0.8200 - val_loss: 0.3990\n",
      "Epoch 2/5\n",
      "\u001b[1m150/150\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m50s\u001b[0m 330ms/step - accuracy: 0.8645 - loss: 0.3250 - val_accuracy: 0.8462 - val_loss: 0.3691\n",
      "Epoch 3/5\n",
      "\u001b[1m150/150\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m51s\u001b[0m 338ms/step - accuracy: 0.9080 - loss: 0.2384 - val_accuracy: 0.8383 - val_loss: 0.3846\n",
      "Epoch 4/5\n",
      "\u001b[1m150/150\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m51s\u001b[0m 339ms/step - accuracy: 0.9292 - loss: 0.1933 - val_accuracy: 0.8308 - val_loss: 0.4250\n",
      "Epoch 5/5\n",
      "\u001b[1m150/150\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m50s\u001b[0m 335ms/step - accuracy: 0.9353 - loss: 0.1798 - val_accuracy: 0.8350 - val_loss: 0.4901\n",
      "Epoch 1/5\n",
      "\u001b[1m150/150\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m50s\u001b[0m 333ms/step - accuracy: 0.9512 - loss: 0.1325 - val_accuracy: 0.8221 - val_loss: 0.4737\n",
      "Epoch 2/5\n",
      "\u001b[1m150/150\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m50s\u001b[0m 333ms/step - accuracy: 0.9577 - loss: 0.1200 - val_accuracy: 0.8196 - val_loss: 0.5500\n",
      "Epoch 3/5\n",
      "\u001b[1m150/150\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m49s\u001b[0m 330ms/step - accuracy: 0.9322 - loss: 0.1861 - val_accuracy: 0.8196 - val_loss: 0.5119\n",
      "Epoch 4/5\n",
      "\u001b[1m150/150\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m51s\u001b[0m 339ms/step - accuracy: 0.9557 - loss: 0.1279 - val_accuracy: 0.8150 - val_loss: 0.5723\n",
      "Epoch 5/5\n",
      "\u001b[1m150/150\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m61s\u001b[0m 405ms/step - accuracy: 0.9670 - loss: 0.0949 - val_accuracy: 0.8104 - val_loss: 0.6172\n"
     ]
    }
   ],
   "source": [
    "#fitting the model\n",
    "\n",
    "kindle_model.fit(X_train_seq_trunc, y_train, batch_size=64, epochs=5, validation_data=(X_test_seq_trunc1, y_test1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 60ms/step - accuracy: 0.8291 - loss: 0.4630\n",
      "test accuracy:  0.8374999761581421\n"
     ]
    }
   ],
   "source": [
    "#to evaluate the performance of the model on the test data set the loss and accuracy are calculated\n",
    "score, accu = kindle_model.evaluate(X_test_seq_trunc, y_test, batch_size=64)\n",
    "print('test accuracy: ', accu)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The accuracy of this model is now obtained which is around 0.84. This indicated that our simple model is fairly good at gauging the customer sentiment just from the text written in the review. Such a model would be very useful to make useful and practical product decisions for the business to make sure they are adhering to the feedback received from the customers in general. Specifically in this case of Amazon Kindle, such a ML model would help simply and understand customer sentiment for different genres of book and along with the demographic information of the customer can aid make informed business decisions. \n",
    "\n",
    "As for the model and its evaluation, a lot more can be build up on this project like using different models and playing around with the hyperparameters like epochs, maximum number of word etc. Also for the evaluation of the model a confusion matrix or a loss function curve can be computed as well to help visualize the fitting better. However for the sake of this project I have decided to keep it simple and concise. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
